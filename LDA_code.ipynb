{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68eb030a-7611-4bd0-992c-b947b7dcdcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading libraries and data...\n",
      "✔ Data loaded successfully (20,000 samples)\n",
      "New shape: (20000, 1)\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1288629</th>\n",
       "      <td>There are unknown hard inquiries on my credit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783591</th>\n",
       "      <td>I recently reviewed a copy of my credit report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392978</th>\n",
       "      <td>I am a little confused. Between my wife and I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713857</th>\n",
       "      <td>I understand the importance of removing any in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271522</th>\n",
       "      <td>i am filing a complaint because this creditor ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Consumer complaint narrative\n",
       "1288629  There are unknown hard inquiries on my credit ...\n",
       "783591   I recently reviewed a copy of my credit report...\n",
       "392978   I am a little confused. Between my wife and I ...\n",
       "713857   I understand the importance of removing any in...\n",
       "271522   i am filing a complaint because this creditor ..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Load libraries and data (with sampling)\n",
    "print(\"1. Loading libraries and data...\")\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "file_path = r\"C:/Users/HP/Downloads/LDA/Complaints_original.csv\"\n",
    "complaint_df = pd.read_csv(file_path).sample(n=20000, random_state=42)\n",
    "\n",
    "print(\"✔ Data loaded successfully (20,000 samples)\")\n",
    "print(f\"New shape: {complaint_df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "complaint_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0119768e-7909-4483-b3e9-f0d557302583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Setting up stop words...\n",
      "✔ Stop words configured (Total: 336)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Configure stop words\n",
    "print(\"2. Setting up stop words...\")\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "stop_words = set(ENGLISH_STOP_WORDS)\n",
    "custom_stopwords = [\n",
    "    \"gimme\", \"lemme\", \"cause\", \"'cuz\", \"imma\", \"gonna\", \"wanna\",\n",
    "    \"gotta\", \"hafta\", \"woulda\", \"coulda\", \"shoulda\", \"howdy\", \"day\",\n",
    "    \"company\", \"bank\", \"hour\", \"express\"\n",
    "]\n",
    "\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Add Product/Sub-product unique values if present\n",
    "if 'Sub-product' in complaint_df.columns:\n",
    "    stop_words.update(complaint_df['Sub-product'].dropna().unique().tolist())\n",
    "if 'Product' in complaint_df.columns:\n",
    "    stop_words.update(complaint_df['Product'].dropna().unique().tolist())\n",
    "\n",
    "print(f\"✔ Stop words configured (Total: {len(stop_words)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2504e9-334a-4227-a964-7b343e3213c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Loading NLP model...\n",
      "✔ spaCy model ready\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize NLP processor\n",
    "print(\"3. Loading NLP model...\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "pos_filter = {\"AUX\", \"PART\", \"PRON\", \"SYM\", \"X\"}\n",
    "\n",
    "print(\"✔ spaCy model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbcfd06-92be-49ce-a756-f9d24b55a491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Defining text cleaner...\n",
      "✔ Cleaning function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Text cleaning function\n",
    "print(\"4. Defining text cleaner...\")\n",
    "\n",
    "# First initialize all required components\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define POS filter (keep these parts of speech)\n",
    "pos_filter = {'PUNCT', 'SYM', 'SPACE', 'X', 'CCONJ', 'DET', 'NUM'}\n",
    "\n",
    "# Get stop words\n",
    "stop_words = set(STOP_WORDS)\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    doc = nlp(str(text))\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.pos_ not in pos_filter and token.text.lower() not in stop_words:\n",
    "            term = token.lemma_.strip() if token.lemma_ != '-PRON-' else token.text\n",
    "            term = term.replace('X', '').replace('/', '').strip()\n",
    "            if len(term) > 1:\n",
    "                tokens.append(term.lower())\n",
    "    return tokens\n",
    "\n",
    "print(\"✔ Cleaning function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655c58c-6429-4852-9bf9-34260d124c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Processing documents...\n",
      "Working with 20000 documents\n",
      "Tokenizing (this may take a while)...\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Process documents\n",
    "print(\"5. Processing documents...\")\n",
    "complaint_df = complaint_df.dropna(subset=['Consumer complaint narrative'])\n",
    "print(f\"Working with {len(complaint_df)} documents\")\n",
    "\n",
    "print(\"Tokenizing (this may take a while)...\")\n",
    "tokenized_docs = [clean_and_tokenize(doc) for doc in complaint_df['Consumer complaint narrative']]\n",
    "\n",
    "print(\"✔ Tokenization complete\")\n",
    "print(f\"Sample tokens: {tokenized_docs[0][:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1316ec9a-dacc-4b0e-8629-a44e563c1587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. Building dictionary...\n",
      "Initial dictionary size: 8434\n",
      "Creating corpus...\n",
      "✔ Corpus created (2894 documents)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create dictionary\n",
    "print(\"6. Building dictionary...\")\n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "print(f\"Initial dictionary size: {len(dictionary)}\")\n",
    "\n",
    "# Optional filtering (uncomment if needed)\n",
    "# dictionary.filter_extremes(no_below=10, no_above=0.3)\n",
    "# print(f\"Filtered dictionary size: {len(dictionary)}\")\n",
    "\n",
    "print(\"Creating corpus...\")\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "print(f\"✔ Corpus created ({len(corpus)} documents)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6813169-7418-4220-b902-d016746ed78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Training LDA model...\n",
      "Training with 10 topics (50 passes)...\n",
      "✔ Model trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Train LDA model\n",
    "print(\"7. Training LDA model...\")\n",
    "from gensim.models import LdaMulticore\n",
    "num_topics = 10\n",
    "\n",
    "print(f\"Training with {num_topics} topics (50 passes)...\")\n",
    "lda_model = LdaMulticore(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    passes=50,\n",
    "    workers=2\n",
    ")\n",
    "print(\"✔ Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65a9970b-6b7b-4a60-9f55-9c9f12d7a402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Extracting topics...\n",
      "\n",
      "Discovered topics:\n",
      "Topic 0: payment late report\n",
      "Topic 1: report account information\n",
      "Topic 2: debt collection provide\n",
      "Topic 3: report account consumer\n",
      "Topic 4: account check money\n",
      "Topic 5: consumer 15 information\n",
      "Topic 6: receive mortgage send\n",
      "Topic 7: payment loan pay\n",
      "Topic 8: xxxx xxxxxxxx date\n",
      "Topic 9: account identity report\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Show topics\n",
    "print(\"8. Extracting topics...\")\n",
    "topics = lda_model.show_topics(num_topics=num_topics, num_words=3, formatted=False)\n",
    "\n",
    "print(\"\\nDiscovered topics:\")\n",
    "for topic_id, words in topics:\n",
    "    print(f\"Topic {topic_id}: {' '.join([w for w,_ in words])}\")\n",
    "\n",
    "topic_labels = {\n",
    "    topic_id: \" \".join([word for word, _ in word_probs])\n",
    "    for topic_id, word_probs in topics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840c910b-5ded-47e9-b9b2-e33d777390c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Assigning topics to documents...\n",
      "✔ Topic assignment complete\n",
      "Topic Label column exists: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Assign topics (Fixed Version)\n",
    "print(\"9. Assigning topics to documents...\")\n",
    "\n",
    "# Get document topics from the model\n",
    "doc_topics = lda_model[corpus]\n",
    "\n",
    "# Create topic labels dictionary (from Cell 8)\n",
    "topic_labels = {\n",
    "    topic_id: \" \".join([word for word, _ in lda_model.show_topic(topic_id, topn=3)])\n",
    "    for topic_id in range(lda_model.num_topics)\n",
    "}\n",
    "\n",
    "# Assign dominant topic to each document\n",
    "complaint_df = complaint_df.copy()  # Ensure we're working on a copy\n",
    "complaint_df['Topic_Label'] = [\n",
    "    topic_labels[max(doc, key=lambda x: x[1])[0]] if doc else \"No Topic\"\n",
    "    for doc in doc_topics\n",
    "]\n",
    "\n",
    "print(\"✔ Topic assignment complete\")\n",
    "print(f\"Topic Label column exists: {'Topic_Label' in complaint_df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab6f1fdc-94d4-49d6-9446-54ff251d2b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. Sample results:\n",
      "\n",
      "Complaint (Topic: payment late report):\n",
      "15 USC 1681 a ( d ) ( 2 ) ( A ) ( i ) clearly states that transactions between the consumer and the Person/Corporation making the report is NOT included on the consumer reports. Yet XXXX, Experian & X...\n",
      "\n",
      "Complaint (Topic: payment loan pay):\n",
      "I tried to transfer my school loans from Navient to XXXX in XX/XX/XXXX because I was making payments but was unable to get out of just paying interest so it felt like I was n't getting anywhere with t...\n",
      "\n",
      "Complaint (Topic: account check money):\n",
      "When I purchase my original item ( mini red light treatment ) with my credit wise Wells Fargo card for {$650.00}. I decided to upgrade my purchase with a promo special that the merchant offered. I did...\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: View results (Guaranteed to Work)\n",
    "print(\"10. Sample results:\")\n",
    "\n",
    "# Verify the column exists first\n",
    "if 'Topic_Label' not in complaint_df.columns:\n",
    "    raise ValueError(\"'Topic_Label' column missing - check Cell 9\")\n",
    "\n",
    "# Safe display\n",
    "sample = complaint_df.sample(3)\n",
    "for _, row in sample.iterrows():\n",
    "    print(f\"\\nComplaint (Topic: {row['Topic_Label']}):\")\n",
    "    print(str(row['Consumer complaint narrative'])[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e330a035-e39d-4553-9442-67bee0af1b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving topics to text file...\n",
      "✔ Saved 2894 topics to C:/Users/HP/Downloads/LDA/FINAL_RESULT.txt\n",
      "File format verification:\n",
      "First 3 lines:\n",
      "report account information\n",
      "receive mortgage send\n",
      "debt collection provide\n"
     ]
    }
   ],
   "source": [
    "# Save topics to text file (add this after Cell 10)\n",
    "output_file = \"C:/Users/HP/Downloads/LDA/FINAL_RESULT.txt\"\n",
    "\n",
    "print(\"\\nSaving topics to text file...\")\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    # Write one topic per line\n",
    "    for topic in complaint_df['Topic_Label']:\n",
    "        f.write(f\"{topic}\\n\")  # No extra spaces or formatting\n",
    "\n",
    "print(f\"✔ Saved {len(complaint_df)} topics to {output_file}\")\n",
    "print(\"File format verification:\")\n",
    "print(\"First 3 lines:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    for _ in range(3):\n",
    "        print(f.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0a1e522-d202-4e44-b495-86b6b1f31093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12. Calculating metrics in specified format...\n",
      "✔ Metrics saved to 'lda_metrics_exact.txt'\n",
      "\n",
      "========================================\n",
      "Topics evaluated: 2894\n",
      "Text documents: 2894\n",
      "Dictionary size: 8434\n",
      "\n",
      "COHERENCE SCORES:\n",
      "      C_V: 0.6071\n",
      "   U_MASS: -1.2850\n",
      "   C_NPMI: 0.0767\n",
      "    C_UCI: 0.3938\n",
      "\n",
      "DIVERSITY SCORES:\n",
      "  topic_diversity: 0.3050\n",
      "  avg_pairwise_jaccard_distance: 0.8948\n",
      "\n",
      "QUALITY SCORES:\n",
      "  num_topics: 10\n",
      "  coverage: 1.0000\n",
      "  avg_topic_size: 289.4000\n",
      "  std_topic_size: 166.2992\n",
      "  topic_size_entropy: 2.1495\n",
      "  largest_topic_ratio: 0.2087\n",
      "  avg_confidence: 0.6436\n",
      "  avg_prob_entropy: 0.8001\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Calculate and Save Metrics (Exact Format)\n",
    "print(\"\\n12. Calculating metrics in specified format...\")\n",
    "\n",
    "from gensim.models import CoherenceModel\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_exact_metrics(model, corpus, dictionary, tokenized_docs, num_topics):\n",
    "    \"\"\"Calculate metrics in the exact requested format\"\"\"\n",
    "    # Basic counts\n",
    "    num_docs = len(tokenized_docs)\n",
    "    dict_size = len(dictionary)\n",
    "    \n",
    "    # Get topics in correct format for coherence calculations\n",
    "    topics = model.show_topics(num_topics=num_topics, formatted=False)\n",
    "    topic_words = [[word for word, _ in topic[1]] for topic in topics]\n",
    "    \n",
    "    # Coherence Scores\n",
    "    cv = CoherenceModel(topics=topic_words,\n",
    "                      texts=tokenized_docs,\n",
    "                      dictionary=dictionary,\n",
    "                      coherence='c_v').get_coherence()\n",
    "    \n",
    "    umass = CoherenceModel(topics=topic_words,\n",
    "                         texts=tokenized_docs,\n",
    "                         dictionary=dictionary,\n",
    "                         coherence='u_mass').get_coherence()\n",
    "    \n",
    "    npmi = CoherenceModel(topics=topic_words,\n",
    "                        texts=tokenized_docs,\n",
    "                        dictionary=dictionary,\n",
    "                        coherence='c_npmi').get_coherence()\n",
    "    \n",
    "    uci = CoherenceModel(topics=topic_words,\n",
    "                       texts=tokenized_docs,\n",
    "                       dictionary=dictionary,\n",
    "                       coherence='c_uci').get_coherence()\n",
    "\n",
    "    # Topic Diversity\n",
    "    unique_words = len(set(word for topic in topic_words for word in topic))\n",
    "    diversity = unique_words / (num_topics * 20)\n",
    "    \n",
    "    # Jaccard Distance\n",
    "    jaccard_dists = []\n",
    "    for i in range(num_topics):\n",
    "        for j in range(i+1, num_topics):\n",
    "            set1 = set(topic_words[i])\n",
    "            set2 = set(topic_words[j])\n",
    "            intersection = len(set1 & set2)\n",
    "            union = len(set1 | set2)\n",
    "            jaccard_dists.append(1 - (intersection / union) if union else 0)\n",
    "    avg_jaccard = np.mean(jaccard_dists) if jaccard_dists else 0\n",
    "\n",
    "    # Topic Distribution Stats\n",
    "    topic_counts = np.zeros(num_topics)\n",
    "    for doc in model[corpus]:\n",
    "        if doc:\n",
    "            topic_id, _ = max(doc, key=lambda x: x[1])\n",
    "            topic_counts[topic_id] += 1\n",
    "    \n",
    "    avg_size = np.mean(topic_counts)\n",
    "    std_size = np.std(topic_counts)\n",
    "    size_entropy = entropy(topic_counts[topic_counts > 0])\n",
    "    max_ratio = np.max(topic_counts) / num_docs if num_docs else 0\n",
    "\n",
    "    # Confidence and Entropy\n",
    "    confidences = []\n",
    "    entropies = []\n",
    "    for doc in model[corpus]:\n",
    "        if doc:\n",
    "            probs = np.array([prob for _, prob in doc])\n",
    "            confidences.append(np.max(probs))\n",
    "            entropies.append(entropy(probs))\n",
    "    \n",
    "    avg_conf = np.mean(confidences) if confidences else 0\n",
    "    avg_ent = np.mean(entropies) if entropies else 0\n",
    "\n",
    "    # Calculate coverage safely\n",
    "    coverage = len(corpus)/num_docs if num_docs else 0\n",
    "    \n",
    "    return f\"\"\"Topics evaluated: {num_docs}\n",
    "Text documents: {num_docs}\n",
    "Dictionary size: {dict_size}\n",
    "\n",
    "COHERENCE SCORES:\n",
    "      C_V: {cv:.4f}\n",
    "   U_MASS: {umass:.4f}\n",
    "   C_NPMI: {npmi:.4f}\n",
    "    C_UCI: {uci:.4f}\n",
    "\n",
    "DIVERSITY SCORES:\n",
    "  topic_diversity: {diversity:.4f}\n",
    "  avg_pairwise_jaccard_distance: {avg_jaccard:.4f}\n",
    "\n",
    "QUALITY SCORES:\n",
    "  num_topics: {num_topics}\n",
    "  coverage: {coverage:.4f}\n",
    "  avg_topic_size: {avg_size:.4f}\n",
    "  std_topic_size: {std_size:.4f}\n",
    "  topic_size_entropy: {size_entropy:.4f}\n",
    "  largest_topic_ratio: {max_ratio:.4f}\n",
    "  avg_confidence: {avg_conf:.4f}\n",
    "  avg_prob_entropy: {avg_ent:.4f}\"\"\"\n",
    "\n",
    "# Calculate and save metrics\n",
    "metrics_report = calculate_exact_metrics(lda_model, corpus, dictionary, tokenized_docs, num_topics)\n",
    "\n",
    "# Save to file with UTF-8 encoding\n",
    "with open(\"lda_metrics_exact.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(metrics_report)\n",
    "\n",
    "# Print to notebook\n",
    "print(\"✔ Metrics saved to 'lda_metrics_exact.txt'\")\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(metrics_report)\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94108c1-797d-4892-9103-f152d25ef1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
